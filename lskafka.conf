# Input from kafka
input {

# input for DataMetrics collection
        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_DataMetrics"
                topic_id => "DataMetrics_SharedDB"
                type => "kafka-datametrics"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_CallStatsData"
                topic_id => "CallStatsData_SharedDB"
                type => "kafka-callstatsdata"
                zk_connect => "zookeeper.service.consul:2181"
        }


        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_LogMetaData"
                topic_id => "LogMetaData_SharedDB"
                type => "kafka-logmetadata"
                zk_connect => "zookeeper.service.consul:2181"
        }

                kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_LogData"
                topic_id => "LogData_SharedDB"
                type => "kafka-logdata"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxtlsaccess"
                topic_id => "nginxtlsaccess"
                type => "nginx-tlsaccess"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxaccess"
                topic_id => "nginxaccess"
                type => "nginx-access"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxbody"
                topic_id => "nginxbody"
                type => "nginx-body"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxerror"
                topic_id => "nginxerror"
                type => "nginx-error"
                zk_connect => "zookeeper.service.consul:2181"
        }


}

filter {
# check to see if there is a json parse failure, if so, let's fix the fields with the messes in them and convert to json
# json parse failures are due to \\x notation in messages which is invalid in JSON
# \\x appears due to people accessing the web interface, or hack attemps
        if "_jsonparsefailure" in [tags] {
                                                mutate { 
                                                        gsub => [ 
# see https://discuss.elastic.co/t/how-to-replace-special-characters-with-a-logstash-filter/28240/2 for the reasoning behind [\\\\]                                                        
                                                                  "message", "[\\\\]", ""
                                                                ] 
                                                }
                                                mutate { remove_tag => [ "tags", "_jsonparsefailure" ] }
                                                # we remove the _jsonparsefailure so that the next json processing generates the tag should the json still not be valid
                                                json { source => "message" }

        }



# copy original timestamp to new field before calling date {} so we can create a logstash lag indicator
        ruby {
                code => "
                                begin
                                        event['logstash_ts'] = event['@timestamp']
                                        rescue Exception => e
                                        event['logstash_ruby_exception'] = '[logstash_ts]: ' + e.message
                                end
                        "
        }


# filter for logs from nginx except for the error log because it does not come in json
        if [type] == "nginx-tlsaccess" or [type] == "nginx-access" or [type] == "nginx-body" {

                                        # add geo ip tags to nginx logs
                                        geoip {
                                                source => "remote_addr"
                                                target => "geoip"
                                                database => "/etc/logstash/GeoLiteCity.dat"
                                        }
# timestamp processing                                        
                                        if [timestamp] and [timestamp] != "" {
                                                                                date { match=> [ "timestamp", "UNIX", "ISO8601" ]
                                                                                       remove_field => [ "rsys_timestamp", "timestamp" ]
                                                                                }
                                        }

                                        # check upstream server field to see if it is multiple entry array
                                        if [upstream_server] =~ /,/ {
                                                                        # we only care about the first value in the list
                                                                        grok {
                                                                                match => [ "req_upstream_time", '
                                        } else {
#                                                                       # we have only a single value, therefore only a single value to convert
                                                                        mutate { convert => { "req_upstream_time" => "float" } }
                                                                        mutate { convert => { "req_total_time" => "float" } }
                                        }

                                        # calculate nginx server lag
                                        ruby {
                                                code => "
                                                                begin
                                                                        event['nginx_server_lag'] = ( ( event['req_total_time'] - event['req_upstream_time'] ) ).to_f
                                                                        rescue Exception => e
                                                                        event['logstash_ruby_exception'] = '[nginx_server_lag]: ' + e.message
                                                                end
                                                        "
                                        }
        } else if [type] == "nginx-error" {
#                                       mutate { add_field => { "capture_message" => "%{message}" } }
                                        # extract fields from error log message
                                        grok {
                                                match => [ "message", "%{DATE_YMD_TIME:sys_ts}(\s)(?<error_level>(?:(?!\s).)*)(\s)(?<PID>(?:(?!\#).)*)\#(?<TID>(?:(?!\:).)*)\:(\s)\*(?<CID>(?:(?!\s).)*)%{GREEDYDATA:error_message}",
                                                           "message", "%{DATE_YMD_TIME:sys_ts}(\s)(?<error_level>(?:(?!\s).)*)(\s)(?<PID>(?:(?!\#).)*)\#(?<TID>(?:(?!\:).)*)\:%{GREEDYDATA:error_message}" ]
                                                patterns_dir => ["/opt/logstash/patterns" ]
                                        }

                                        date { match => [ "sys_ts", "YYYY/MM/dd HH:mm:ss" ] }


        } else if [type] == "kafka-datametrics" or [type] == "hrlyEvtLvls" or [type] == "hrlyLogLvls" {


                                        #extract date field into sys_ts
                                        mutate { add_field => { "sys_ts" => "%{[ts][$date]}" } }

                                        date { match => [ "sys_ts", "UNIX_MS", "UNIX", "ISO8601" ]
                                        } 
                                        mutate { add_field => { "message_type" => "%{type}" } }
                                        mutate { replace => { "type" => "datametrics" } }
                                        mutate { remove_field => [ "ts" ] }
                                        mutate { remove_field => [ "_id" ] }

        } else if [type] == "kafka-callstatsdata" {
        # build a date field to check against to prevent dates prior to 2015 or later than today from posting invalid log entries
                                                        ruby {
                                                                code => "
                                                                                begin
                                                                                event['dateref'] = (Time.now.to_i - event['@timestamp'].to_i) / 86400
                                                                                rescue Exception => e
                                                                                event['logstash_ruby_exception'] = 'old records: ' + e.message
                                                                                end
                                                                "
                                                        }
        
                                                        date { match => [ "dateref", "UNIX_MS" ] target => "dateref" }


                                        mutate { replace => { "ts" => "%{[ts][$date]}" } }
                                        date {
                                                match => [ "ts", "UNIX_MS" ]
                                                target => "ts"
                                        }
                                        mutate { replace => { "dts" => "%{[dts][$date]}" } }
                                        date {
                                                match => [ "dts", "UNIX_MS" ]
                                                target => "dts"
                                        }
                                        if [dts] < [@metadata][indexfrom] or [dts] > [@metadata][indexto] {
                                                                                # selected date is prior to Janurary 01, 2015 or post today, use Mongo collection date instead
                                                                                mutate { add_field => { "sys_ts" => "%{ts}" } }
                                        } else {
                                                mutate { add_field => { "sys_ts" => "%{dts}" } }
                                        }
                                        date {
                                                match => [ "sys_ts", "ISO8601" ]
                                        }
                                        mutate { remove_field => [ "_id" ] }
                                        if [key] == "callstarted" {
                                                                        mutate { add_tag => [ "callstart" ] }
                                        }

                                        if [key] == "callended" {
                                                                        mutate { add_tag => [ "callend" ] }
                                        }

# use elapsed plugin to calculate call duration field
                                        elapsed {
                                                        start_tag => "callstart"
                                                        end_tag => "callend"
                                                        unique_id_field => "id"
                                                        timeout => 43200
                                        }


        } else if [type] == "kafka-logmetadata" {

                                        #extract date field into sys_ts
                                        #mutate { add_field => { "sys_ts" => "%{[dts][$date]}" } } 
                                        # extract date fields to top level JSON
                                        mutate { replace => { "ts" => "%{[ts][$date]}" } }
                                        date {
                                                match => [ "ts", "UNIX_MS" ]
                                                target => "ts" 
                                        }
                                        mutate { replace => { "vdt" => "%{[vdt][$date]}" } }
                                        date {
                                                match => [ "vdt", "UNIX_MS" ]
                                                target => "vdt"
                                        }
                                        mutate { replace => { "dts" => "%{[dts][$date]}" } }
                                        date {
                                                match => [ "dts", "UNIX_MS" ]
                                                target => "dts"
                                        }
                                        # verify device timestamp (dts) is valid, if not, use server received timestamp
                                        if [dts] < [@metadata][indexfrom] or [dts] > [@metadata][indexto] {
                                                                                # selected date is prior to Janurary 01, 2015 or post today, use Mongo collection date instead
                                                                                mutate { add_field => { "sys_ts" => "%{ts}" } }
                                        } else {
                                                mutate { add_field => { "sys_ts" => "%{dts}" } }
                                        }
                                        date {
                                                match => [ "sys_ts", "ISO8601" ]
                                        }
                                        # compare [sys_ts] timestamp with [ts] timestamp to see how much lag between device log collection and upload into mongo
                                        ruby {
                                                code => "
                                                                begin
                                                                        event['device_to_mongo_lag'] = ( ( event['ts'] - event['@timestamp'] ) ).to_f
                                                                        rescue Exception => e
                                                                        event['logstash_ruby_exception'] = '[device_to_mongo_lag]: ' + e.message
                                                                end
                                                        "
                                        }
#                                        mutate { add_field => { "mongo_id" => "%{_id}" } }
                                        mutate { remove_field => [ "_id" ] }

        } else if [type] == "kafka-logdata" {
# process one timestamp field in very nonstandard format
                                        mutate { add_field => { "temptime" => "%{@timestamp}" } }
                                        # extract date from ts field
                                        date {
                                                match => [ "ts", "YYMMddHHmm" ]
                                                target => "ts"
                                        }

                                        mutate { add_field => { "sys_ts" => "%{ts}" } }
                                        date { match => [ "sys_ts", "ISO8601" ] }


                                        if "_dateparsefailure" not in [tags] {
                                      # verify device timestamp (ts) is valid, if not, use server received timestamp
                                        if [ts] < [@metadata][indexfrom] or [ts] > [@metadata][indexto] {
                                                                                # selected date is prior to Janurary 01, 2015 or post today, use today's timestamp
                                                                                mutate { add_field => { "sys_ts" => "%{temptime}" } }
                                        } else {
                                                mutate { add_field => { "sys_ts" => "%{ts}" } }
                                        }
                                        date {
                                                match => [ "sys_ts", "ISO8601" ]
                                        }

                                        } else {
                                                mutate { remove_tag => [ "_dateparsefailure" ] }
                                                mutate { replace => { "sys_ts" => "%{temptime}" } }
                                                date { match => [ "sys_ts", "ISO8601" ] }
                                        }
#                                        mutate { add_field => { "mongo_id" => "%{_id}" } }
                                        mutate { remove_field => [ "_id" ] }
                                        mutate { remove_field => [ "temptime", "ts" ] }


        }

# if no parse failures occur, remove these fields from being sent
        if "_grokparsefailure" not in [tags] and "_jsonparsefailure" not in [tags] and "_rubyexception" not in [tags] and "_dateparsefailure" not in [tags] {
                                                mutate { remove_field => [ "beat", "input_type", "message", "fields", "comp_message", "org_message", "comp_message", "comp_message2", "request_body_part_a", "request_body_part_b", "request_body", "check-field", "data", "schema", "rsys_timestamp" ] }
        }

# compare the logstash timestamp to the log message timestamp to see how long it took the log message to arrive in logstash
        ruby {
                code => "
                                begin
                                        event['logstash_lag'] = ( ( event['logstash_ts'] - event['@timestamp'] ) ).to_f
                                        rescue Exception => e
                                        event['logstash_ruby_exception'] = '[logstash_lag]: ' + e.message
                                end
                        "
        }

}

output {
# output log message rate to statsd for visualization in Grafana
        statsd {
                host => "logf002"
                increment => "messages.count"
                sender => "logf002"
        }

# if a particular message failed to parse, put it in its own log file for later review, do not send to elasticsearch
        if "_grokparsefailure" in [tags] or "_dateparsefailure" in [tags] or "_jsonparsefailure" in [tags] or "_rubyexception" in [tags] {
                                                                        file { path => "/var/log/logstash/failed-%{type}-events-%{+YYYY-MM-dd}-%{tags}" }

        } else {
# create a journal file of all logs messages that can be scanned for various events and the creation of notifications
#        file {
#               path => "/var/log/logstash/%{type}-%{+YYYY.MM.dd}"
#        }
# send to elasticsearch input node
                        elasticsearch {
                                hosts => [ "nginx-loadbalance-es.service.consul:9200"]
                                index => "%{type}-%{+YYYY.MM.dd}"
                                workers => 2
                        }
        }

}
