# Input from kafka
input {

# input for DataMetrics collection
        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_DataMetrics"
                topic_id => "DataMetrics_SharedDB"
                type => "kafka-datametrics"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_CallStatsData"
                topic_id => "CallStatsData_SharedDB"
                type => "kafka-callstatsdata"
                zk_connect => "zookeeper.service.consul:2181"
        }


        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_LogMetaData"
                topic_id => "LogMetaData_SharedDB"
                type => "kafka-logmetadata"
                zk_connect => "zookeeper.service.consul:2181"
        }

                kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_LogData"
                topic_id => "LogData_SharedDB"
                type => "kafka-logdata"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxtlsaccess"
                topic_id => "nginxtlsaccess"
                type => "nginx-tlsaccess"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxaccess"
                topic_id => "nginxaccess"
                type => "nginx-access"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxbody"
                topic_id => "nginxbody"
                type => "nginx-body"
                zk_connect => "zookeeper.service.consul:2181"
        }

        kafka {
                consumer_id => "logf002"
                consumer_threads => "6"
                group_id => "kafka_nginxerror"
                topic_id => "nginxerror"
                type => "nginx-error"
                zk_connect => "zookeeper.service.consul:2181"
        }


}

filter {
# check to see if there is a json parse failure, if so, let's fix the fields with the messes in them and convert to json
# json parse failures are due to \\x notation in messages which is invalid in JSON
# \\x appears due to people accessing the web interface, or hack attemps
        if "_jsonparsefailure" in [tags] {
                                                mutate { 
                                                        gsub => [ 
# see https://discuss.elastic.co/t/how-to-replace-special-characters-with-a-logstash-filter/28240/2 for the reasoning behind [\\\\]                                                        
                                                                  "message", "[\\\\]", ""
                                                                ] 
                                                }
                                                mutate { remove_tag => [ "tags", "_jsonparsefailure" ] }
                                                # we remove the _jsonparsefailure so that the next json processing generates the tag should the json still not be valid
                                                json { source => "message" }

        }



# copy original timestamp to new field before calling any date {} processing so we can create a logstash lag indicator
        ruby {
                code => "
                                begin
                                        event['logstash_ts'] = event['@timestamp']
                                        rescue Exception => e
                                        event['logstash_ruby_exception'] = '[logstash_ts]: ' + e.message
                                end
                        "
        }


# filter for logs from nginx except for the error log because it cannot be json formatted
        if [type] == "nginx-tlsaccess" or [type] == "nginx-access" or [type] == "nginx-body" {

                                        # add geo ip tags to nginx logs
                                        geoip {
                                                source => "remote_addr"
                                                target => "geoip"
                                                database => "/etc/logstash/GeoLiteCity.dat"
                                        }
                                        # timestamp processing                                        
                                        if [timestamp] and [timestamp] != "" {
                                                                                date { match=> [ "timestamp", "UNIX", "ISO8601" ]
                                                                                       remove_field => [ "rsys_timestamp", "timestamp" ]
                                                                                }
                                        }

                                        # check upstream server field to see if it is multiple entry array
                                        # grab and convert the first value in the list to a float, split out first upstream_server value
                                        grok {
                                                match => [ "req_upstream_time", '%{NUMBER:req_upstream_time:float}(,%{GREEDYDATA:req_upstream_time_remainder})?' ]
                                                overwrite => [ "req_upstream_time" ]
                                        }
                                        mutate { convert => { "req_total_time" => "float" } }
                                        grok {
                                                match => [ "upstream_server", '(?<upstream_server>(?:(?!,).)*)(,%{GREEDYDATA:upstream_server_remainder})?' ]
                                                overwrite => [ "upstream_server" ]
                                        }
                                        

                                        # calculate nginx server lag
                                        ruby {
                                                code => "
                                                                begin
                                                                        event['nginx_server_lag'] = ( ( event['req_total_time'] - event['req_upstream_time'] ) ).to_f
                                                                        rescue Exception => e
                                                                        event['logstash_ruby_exception'] = '[nginx_server_lag]: ' + e.message
                                                                end
                                                        "
                                        }
        } else if [type] == "nginx-error" {
#                                       mutate { add_field => { "capture_message" => "%{message}" } }
                                        # extract fields from error log message
                                        grok {
                                                match => [ "message", "%{DATE_YMD_TIME:sys_ts}(\s)(?<error_level>(?:(?!\s).)*)(\s)(?<PID>(?:(?!\#).)*)\#(?<TID>(?:(?!\:).)*)\:(\s)\*(?<CID>(?:(?!\s).)*)%{GREEDYDATA:error_message}",
                                                           "message", "%{DATE_YMD_TIME:sys_ts}(\s)(?<error_level>(?:(?!\s).)*)(\s)(?<PID>(?:(?!\#).)*)\#(?<TID>(?:(?!\:).)*)\:%{GREEDYDATA:error_message}" ]
                                                patterns_dir => ["/opt/logstash/patterns" ]
                                        }

                                        date { match => [ "sys_ts", "YYYY/MM/dd HH:mm:ss" ]
                                                remove_field => [ "sys_ts" ]
                                        }


        } else if [type] == "kafka-datametrics" or [type] == "hrlyEvtLvls" or [type] == "hrlyLogLvls" {

                                        date { match => [ "[ts][$date]", "UNIX_MS", "UNIX", "ISO8601" ] } 
                                        mutate { add_field => { "message_type" => "%{type}" }
                                                 replace => { "type" => "datametrics" } 
                                                 remove_field => [ "ts", "_id" ] }
                                        }
                                        
        } else if [type] == "kafka-callstatsdata" {
        # build a date field to check against to prevent dates prior to 2015 or later than today from posting invalid log entries
                                        ruby {
                                                code => "
                                                        begin
                                                                event['dateref'] = event['@timestamp'].to_f
                                                                rescue Exception => e
                                                                event['logstash_ruby_exception'] = 'old records: ' + e.message
                                                        end
                                                        "
                                        }
        
                                        if [dts][$date] < 1438992000000 or [dts][$date] > [dateref] {
                                                                                # selected date is prior to August 8, 2015 or greater than today, use Mongo collection date instead
                                                                                date { match => [ "[ts][$date]", "UNIX_MS" ] remove_field => [ "_id", "sys_ts" ] }
                                        } else {
                                                date { match => [ "[dts][$date]", "UNIX_MS" ] remove_field => [ "_id", "sys_ts" ] }
                                        }

                                        if [key] == "callstarted" {
                                                                        mutate { add_tag => [ "callstart" ] }
                                        } else if [key] == "callended" {
                                                                        mutate { add_tag => [ "callend" ] }
                                        }

                                        # use elapsed plugin to calculate call duration field
                                        elapsed {
                                                        start_tag => "callstart"
                                                        end_tag => "callend"
                                                        unique_id_field => "id"
                                                        timeout => 43200
                                        }


        } else if [type] == "kafka-logmetadata" {

                                        ruby {
                                                code => "
                                                        begin
                                                                event['dateref'] = event['@timestamp'].to_f
                                                                rescue Exception => e
                                                                event['logstash_ruby_exception'] = 'old records: ' + e.message
                                                        end
                                                        "
                                        }
        
                                        if [dts][$date] < 1438992000000 or [dts][$date] > [dateref] {
                                                                                # selected date is prior to August 8, 2015 or greater than today, use Mongo collection date instead
                                                                                date { match => [ "[ts][$date]", "UNIX_MS" ] remove_field => [ "_id" ] }
                                        } else {
                                                date { match => [ "[dts][$date]", "UNIX_MS" ] remove_field => [ "_id" ] }
                                        }

                                        # compare @timestamp with [ts] timestamp to see how much lag between device log collection and upload into mongo
                                        date { match => [ "[ts][$date]", "UNIX_MS" ] target => "ts" }
                                        ruby {
                                                code => "
                                                                begin
                                                                        event['device_to_mongo_lag'] = ( ( event['ts'] - event['@timestamp'] ) ).to_f
                                                                        rescue Exception => e
                                                                        event['logstash_ruby_exception'] = '[device_to_mongo_lag]: ' + e.message
                                                                end
                                                        "
                                        }
                                        mutate { remove_field => [ "ts", "dts" ]
        } else if [type] == "kafka-logdata" {

                                        # extract date from [ts] field if [ts] is not empty
                                        if [ts] and [ts] != "" {
                                                                # if date field not empty, convert to ISO8601 formatted date
                                                                date { match => [ "ts", "YYMMddHHmm" ] }
                                                                # check to see if date is a valid date in the system
                                                                mutate {
                                                                        add_field => { "[@metadata][indexfrom]" => "2015-01-01T00:00:00.000Z"
                                                                }
                                                                date {
                                                                        #convert startdate from a string to a date
                                                                        match => [ "[@metadata][indexfrom]" , "ISO8601" ]
                                                                        target => "[@metadata][indexfrom]"
                                                                }
                                                                # if [ts] falls outside the accepted dates, use system timestamp instead
                                                                if [ts] < [@metadata][indexfrom] or [ts] > [logstash_ts] {
                                                                                            # selected date is prior to August, 2015 or post today, use today's timestamp
                                                                                            date { match => [ "[logstash_ts]", "ISO8601" ] }
                                                                } 
                                                                

                                        } else {
                                                # if [ts] is blank, leave the timestamp as the system timestamp
                                        }
                                        mutate { remove_field => [ "ts" ] }
        }

# if no parse failures occur, remove these fields from being sent
        if "_grokparsefailure" not in [tags] and "_jsonparsefailure" not in [tags] and "_rubyexception" not in [tags] and "_dateparsefailure" not in [tags] {
                                                mutate { remove_field => [ "beat", "input_type", "message", "fields", "comp_message", "org_message", "comp_message", "comp_message2", "request_body_part_a", "request_body_part_b", "request_body", "check-field", "data", "schema", "rsys_timestamp" ] }
        }

# compare the logstash timestamp to the log message timestamp to see how long it took the log message to arrive in logstash
        ruby {
                code => "
                                begin
                                        event['logstash_lag'] = ( ( event['logstash_ts'] - event['@timestamp'] ) ).to_f
                                        rescue Exception => e
                                        event['logstash_ruby_exception'] = '[logstash_lag]: ' + e.message
                                end
                        "
        }

}

output {
# output log message rate to statsd for visualization in Grafana
        statsd {
                host => "logf002"
                increment => "messages.count"
                sender => "logf002"
        }

# if a particular message failed to parse, put it in its own log file for later review, do not send to elasticsearch
        if "_grokparsefailure" in [tags] or "_dateparsefailure" in [tags] or "_jsonparsefailure" in [tags] or "_rubyexception" in [tags] {
                                                                        file { path => "/var/log/logstash/failed-%{type}-events-%{+YYYY-MM-dd}-%{tags}" }

        } else {
# create a journal file of all logs messages that can be scanned for various events and the creation of notifications
#        file {
#               path => "/var/log/logstash/%{type}-%{+YYYY.MM.dd}"
#        }
# send to elasticsearch input node
                        elasticsearch {
                                hosts => [ "nginx-loadbalance-es.service.consul:9200"]
                                index => "%{type}-%{+YYYY.MM.dd}"
                                workers => 2
                        }
        }

}
